// Chapter 10: Warp-specialized pipeline (enhanced) baseline.
//
// Baseline implementation for the adaptive warp-specialized pipeline example.
// Uses the same workload as optimized_warp_specialized_pipeline_enhanced.cu
// but performs synchronous global->shared copies with block-wide barriers.
//
// This keeps the workload equivalent while isolating the benefit of
// cuda::pipeline + cuda::memcpy_async double buffering in the optimized variant.

#include <cuda_runtime.h>
#include <cooperative_groups.h>

#include <algorithm>
#include <cstdio>
#include <cstdlib>
#include <numeric>
#include <vector>

#include "../core/common/headers/arch_detection.cuh"
#include "../core/common/headers/cuda_verify.cuh"

namespace cg = cooperative_groups;

#define CUDA_CHECK(call)                                                     \
  do {                                                                       \
    cudaError_t status = (call);                                             \
    if (status != cudaSuccess) {                                             \
      std::fprintf(stderr, "CUDA error %s:%d: %s\n", __FILE__, __LINE__,     \
                    cudaGetErrorString(status));                            \
      std::exit(EXIT_FAILURE);                                               \
    }                                                                        \
  } while (0)

template <int TILE>
__device__ void compute_tile(const float* a, const float* b, float* c, int lane) {
  constexpr int TILE_ELEMS = TILE * TILE;
  for (int idx = lane; idx < TILE_ELEMS; idx += warpSize) {
    float x = a[idx];
    float y = b[idx];
    c[idx] = sqrtf(x * x + y * y);
  }
}

template <int TILE>
__global__ void warp_specialized_baseline_kernel(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    int total_tiles) {
  constexpr int TILE_ELEMS = TILE * TILE;

  cg::thread_block block = cg::this_thread_block();

  extern __shared__ float smem[];
  float* tile_a = smem;
  float* tile_b = tile_a + TILE_ELEMS;
  float* tile_c = tile_b + TILE_ELEMS;

  int warp_id = threadIdx.x / warpSize;
  int lane = threadIdx.x % warpSize;

  int stride = gridDim.x;
  for (int tile = blockIdx.x; tile < total_tiles; tile += stride) {
    size_t offset = static_cast<size_t>(tile) * TILE_ELEMS;

    if (warp_id == 0) {
      for (int idx = lane; idx < TILE_ELEMS; idx += warpSize) {
        tile_a[idx] = A[offset + idx];
        tile_b[idx] = B[offset + idx];
      }
    }

    block.sync();

    if (warp_id == 1) {
      compute_tile<TILE>(tile_a, tile_b, tile_c, lane);
    }

    block.sync();

    if (warp_id == 2) {
      for (int idx = lane; idx < TILE_ELEMS; idx += warpSize) {
        C[offset + idx] = tile_c[idx];
      }
    }

    block.sync();
  }
}

template <int TILE>
void run_warp_specialized_baseline(int tiles,
                                   const std::vector<float>& h_A,
                                   const std::vector<float>& h_B,
                                   std::vector<float>& h_C) {
  constexpr int TILE_ELEMS = TILE * TILE;
  size_t elems = static_cast<size_t>(tiles) * TILE_ELEMS;
  size_t bytes = elems * sizeof(float);

  float *d_A = nullptr, *d_B = nullptr, *d_C = nullptr;
  CUDA_CHECK(cudaMalloc(&d_A, bytes));
  CUDA_CHECK(cudaMalloc(&d_B, bytes));
  CUDA_CHECK(cudaMalloc(&d_C, bytes));
  CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), bytes, cudaMemcpyHostToDevice));
  CUDA_CHECK(cudaMemcpy(d_B, h_B.data(), bytes, cudaMemcpyHostToDevice));

  dim3 block(96);
  dim3 grid(std::min(tiles, 256));
  size_t shared_bytes = 3 * TILE_ELEMS * sizeof(float);

  // Warmup
  warp_specialized_baseline_kernel<TILE><<<grid, block, shared_bytes>>>(d_A, d_B, d_C, tiles);
  CUDA_CHECK(cudaGetLastError());
  CUDA_CHECK(cudaDeviceSynchronize());

  // Timed run
  cudaEvent_t start, stop;
  CUDA_CHECK(cudaEventCreate(&start));
  CUDA_CHECK(cudaEventCreate(&stop));

  constexpr int iterations = 10;
  CUDA_CHECK(cudaEventRecord(start));
  for (int i = 0; i < iterations; ++i) {
    warp_specialized_baseline_kernel<TILE><<<grid, block, shared_bytes>>>(d_A, d_B, d_C, tiles);
  }
  CUDA_CHECK(cudaEventRecord(stop));
  CUDA_CHECK(cudaEventSynchronize(stop));

  float elapsed_ms = 0.0f;
  CUDA_CHECK(cudaEventElapsedTime(&elapsed_ms, start, stop));
  float avg_ms = elapsed_ms / iterations;
  std::printf("Kernel time: %.4f ms\n", avg_ms);

  CUDA_CHECK(cudaEventDestroy(start));
  CUDA_CHECK(cudaEventDestroy(stop));
  CUDA_CHECK(cudaMemcpy(h_C.data(), d_C, bytes, cudaMemcpyDeviceToHost));

#ifdef VERIFY
  double checksum = 0.0;
  for (float v : h_C) {
    checksum += static_cast<double>(v);
  }
  VERIFY_PRINT_CHECKSUM(static_cast<float>(checksum));
#endif

  CUDA_CHECK(cudaFree(d_A));
  CUDA_CHECK(cudaFree(d_B));
  CUDA_CHECK(cudaFree(d_C));
}

int main() {
  const auto& limits = cuda_arch::get_architecture_limits();
  if (!limits.supports_clusters) {
    std::printf("⚠️  Skipping warp-specialized pipeline: device lacks cluster/pipeline support.\n");
    return 0;
  }

  int tile = cuda_arch::select_square_tile_size<float>(
      /*shared_tiles=*/3, {32, 16, 8});

  // Use a large tile count so the block-strided loop runs long enough for
  // the optimized double-buffered pipeline to hide global-memory latency.
  int tiles = std::min(16384, std::max(4096, limits.max_cluster_size * 512));
  size_t elems = static_cast<size_t>(tiles) * tile * tile;

  std::vector<float> h_A(elems), h_B(elems), h_C(elems), h_ref(elems);
  std::iota(h_A.begin(), h_A.end(), 0.0f);
  std::iota(h_B.begin(), h_B.end(), 1.0f);

  switch (tile) {
    case 32:
      run_warp_specialized_baseline<32>(tiles, h_A, h_B, h_C);
      break;
    case 16:
      run_warp_specialized_baseline<16>(tiles, h_A, h_B, h_C);
      break;
    default:
      run_warp_specialized_baseline<8>(tiles, h_A, h_B, h_C);
      break;
  }

  for (size_t i = 0; i < elems; ++i) {
    h_ref[i] = std::sqrt(h_A[i] * h_A[i] + h_B[i] * h_B[i]);
  }

  double max_err = 0.0;
  for (size_t i = 0; i < elems; ++i) {
    max_err = std::max(max_err, static_cast<double>(std::abs(h_C[i] - h_ref[i])));
  }
  std::printf("Selected tile size: %d (shared-memory budget %.1f KB)\n",
              tile, limits.max_shared_mem_per_block / 1024.0);
  std::printf("Max error: %.6e\n", max_err);

  return 0;
}
