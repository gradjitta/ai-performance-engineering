"""
torch.compile Diagnostic Capture

Capture and analyze torch.compile behavior:
- Graph breaks
- Fusion analysis  
- Compilation time
- Generated code inspection
"""

import json
import logging
import os
import re
import tempfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

import torch


@dataclass
class CompileBreak:
    """A graph break in torch.compile."""
    reason: str
    location: str
    line_number: int = 0
    suggestion: str = ""


@dataclass
class FusedKernel:
    """A fused kernel generated by torch.compile."""
    name: str
    ops_fused: List[str]
    input_shapes: List[Tuple[int, ...]]
    output_shape: Tuple[int, ...]
    estimated_flops: int = 0


@dataclass
class CompileReport:
    """Complete torch.compile analysis report."""
    
    # Compilation info
    compile_time_ms: float = 0
    backend: str = "inductor"
    mode: str = "default"
    
    # Graph breaks
    graph_breaks: List[CompileBreak] = field(default_factory=list)
    total_graph_breaks: int = 0
    
    # Fusion analysis
    fused_kernels: List[FusedKernel] = field(default_factory=list)
    ops_before_fusion: int = 0
    ops_after_fusion: int = 0
    fusion_ratio: float = 0
    
    # Memory analysis
    peak_memory_mb: float = 0
    memory_savings_mb: float = 0
    
    # Performance comparison
    eager_time_ms: float = 0
    compiled_time_ms: float = 0
    speedup: float = 0
    
    # Recommendations
    recommendations: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "compilation": {
                "compile_time_ms": self.compile_time_ms,
                "backend": self.backend,
                "mode": self.mode,
            },
            "graph_breaks": {
                "total": self.total_graph_breaks,
                "breaks": [
                    {"reason": b.reason, "location": b.location, "suggestion": b.suggestion}
                    for b in self.graph_breaks[:10]
                ],
            },
            "fusion": {
                "ops_before": self.ops_before_fusion,
                "ops_after": self.ops_after_fusion,
                "ratio": self.fusion_ratio,
                "kernels": len(self.fused_kernels),
            },
            "performance": {
                "eager_time_ms": self.eager_time_ms,
                "compiled_time_ms": self.compiled_time_ms,
                "speedup": self.speedup,
            },
            "memory": {
                "peak_mb": self.peak_memory_mb,
                "savings_mb": self.memory_savings_mb,
            },
            "recommendations": self.recommendations,
        }


class TorchCompileAnalyzer:
    """
    Analyze torch.compile behavior and performance.
    
    Usage:
        analyzer = TorchCompileAnalyzer()
        
        report = analyzer.analyze(model, sample_input)
        print(report.speedup)
        print(report.graph_breaks)
    """
    
    def __init__(
        self,
        backend: str = "inductor",
        mode: str = "default",
        output_dir: Optional[Path] = None,
        capture_logs: bool = True,
    ):
        self.backend = backend
        self.mode = mode
        self.output_dir = Path(output_dir) if output_dir else Path("./compile_analysis")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.capture_logs = capture_logs
    
    def analyze(
        self,
        model_or_fn: Callable,
        sample_inputs: Any,
        warmup: int = 3,
        iterations: int = 10,
    ) -> CompileReport:
        """
        Analyze torch.compile behavior on a model or function.
        
        Args:
            model_or_fn: Model or function to analyze
            sample_inputs: Sample inputs for the model
            warmup: Warmup iterations
            iterations: Benchmark iterations
            
        Returns:
            CompileReport with analysis
        """
        report = CompileReport(backend=self.backend, mode=self.mode)
        
        # Prepare inputs
        if not isinstance(sample_inputs, (list, tuple)):
            sample_inputs = (sample_inputs,)
        
        # 1. Benchmark eager mode
        report.eager_time_ms = self._benchmark(model_or_fn, sample_inputs, warmup, iterations)
        
        # 2. Analyze compilation with explain
        explain_output = self._capture_explain(model_or_fn, sample_inputs)
        self._parse_explain_output(explain_output, report)
        
        # 3. Compile and benchmark
        import time
        compile_start = time.perf_counter()
        
        compiled_fn = torch.compile(
            model_or_fn,
            backend=self.backend,
            mode=self.mode,
            fullgraph=False,  # Allow graph breaks for analysis
        )
        
        # Warm up to trigger compilation
        for _ in range(warmup):
            if isinstance(sample_inputs, dict):
                compiled_fn(**sample_inputs)
            else:
                compiled_fn(*sample_inputs)
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        compile_end = time.perf_counter()
        report.compile_time_ms = (compile_end - compile_start) * 1000
        
        # 4. Benchmark compiled
        report.compiled_time_ms = self._benchmark(compiled_fn, sample_inputs, 0, iterations)
        
        # 5. Calculate speedup
        if report.compiled_time_ms > 0:
            report.speedup = report.eager_time_ms / report.compiled_time_ms
        
        # 6. Memory analysis
        if torch.cuda.is_available():
            report.peak_memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        # 7. Generate recommendations
        report.recommendations = self._generate_recommendations(report)
        
        return report
    
    def _benchmark(
        self,
        fn: Callable,
        inputs: Any,
        warmup: int,
        iterations: int,
    ) -> float:
        """Benchmark a function."""
        import time
        
        # Warmup
        for _ in range(warmup):
            if isinstance(inputs, dict):
                fn(**inputs)
            else:
                fn(*inputs)
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        # Timed iterations
        if torch.cuda.is_available():
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)
            
            start.record()
            for _ in range(iterations):
                if isinstance(inputs, dict):
                    fn(**inputs)
                else:
                    fn(*inputs)
            end.record()
            
            torch.cuda.synchronize()
            return start.elapsed_time(end) / iterations
        else:
            start = time.perf_counter()
            for _ in range(iterations):
                if isinstance(inputs, dict):
                    fn(**inputs)
                else:
                    fn(*inputs)
            end = time.perf_counter()
            return (end - start) / iterations * 1000
    
    def _capture_explain(
        self,
        model_or_fn: Callable,
        sample_inputs: Any,
    ) -> str:
        """Capture torch._dynamo.explain output."""
        output = ""
        
        try:
            explanation = torch._dynamo.explain(model_or_fn)
            
            # Run with inputs to get actual graph breaks
            if isinstance(sample_inputs, dict):
                result = explanation(**sample_inputs)
            else:
                result = explanation(*sample_inputs)
            
            # Get explanation text
            if hasattr(result, '__str__'):
                output = str(result)
            
        except Exception as e:
            output = f"Explain failed: {e}"
        
        return output
    
    def _parse_explain_output(self, output: str, report: CompileReport):
        """Parse torch._dynamo.explain output."""
        
        # Count graph breaks
        break_matches = re.findall(r'Graph break', output, re.IGNORECASE)
        report.total_graph_breaks = len(break_matches)
        
        # Parse individual breaks
        break_pattern = r'Graph break.*?:\s*(.*?)(?:\n|$)'
        for match in re.finditer(break_pattern, output):
            reason = match.group(1).strip()
            report.graph_breaks.append(CompileBreak(
                reason=reason,
                location="",
                suggestion=self._get_break_suggestion(reason),
            ))
        
        # Parse ops info
        ops_pattern = r'(\d+)\s+ops\s+before'
        ops_match = re.search(ops_pattern, output)
        if ops_match:
            report.ops_before_fusion = int(ops_match.group(1))
        
        after_pattern = r'(\d+)\s+ops\s+after'
        after_match = re.search(after_pattern, output)
        if after_match:
            report.ops_after_fusion = int(after_match.group(1))
        
        if report.ops_before_fusion > 0 and report.ops_after_fusion > 0:
            report.fusion_ratio = report.ops_before_fusion / report.ops_after_fusion
    
    def _get_break_suggestion(self, reason: str) -> str:
        """Get suggestion for a graph break reason."""
        reason_lower = reason.lower()
        
        if 'data-dependent' in reason_lower:
            return "Use torch.cond or static shapes if possible"
        elif 'unsupported' in reason_lower:
            return "Check if operation has a supported alternative"
        elif 'print' in reason_lower or 'logging' in reason_lower:
            return "Move print/logging outside the compiled function"
        elif 'assert' in reason_lower:
            return "Use torch._assert or move assertions outside"
        elif 'exception' in reason_lower:
            return "Handle exceptions outside the compiled region"
        elif 'numpy' in reason_lower:
            return "Use PyTorch operations instead of NumPy"
        
        return "Review and refactor to use supported operations"
    
    def _generate_recommendations(self, report: CompileReport) -> List[str]:
        """Generate recommendations based on analysis."""
        recommendations = []
        
        # Graph breaks
        if report.total_graph_breaks > 0:
            recommendations.append(
                f"Found {report.total_graph_breaks} graph breaks. "
                f"Consider using fullgraph=True and fixing breaks for best performance."
            )
        
        # Speedup analysis
        if report.speedup < 1.0:
            recommendations.append(
                f"Compiled code is slower ({report.speedup:.2f}x). "
                "This can happen with small inputs or high compilation overhead. "
                "Consider using mode='reduce-overhead' for inference."
            )
        elif report.speedup < 1.2:
            recommendations.append(
                f"Modest speedup ({report.speedup:.2f}x). "
                "Try mode='max-autotune' for potentially better optimization."
            )
        elif report.speedup >= 2.0:
            recommendations.append(
                f"Excellent speedup ({report.speedup:.2f}x)! "
                "torch.compile is working well for this workload."
            )
        
        # Fusion analysis
        if report.fusion_ratio > 2.0:
            recommendations.append(
                f"Good kernel fusion: {report.ops_before_fusion} ops → {report.ops_after_fusion} "
                f"({report.fusion_ratio:.1f}x reduction)"
            )
        
        # Compile time
        if report.compile_time_ms > 30000:  # > 30 seconds
            recommendations.append(
                f"High compilation time ({report.compile_time_ms/1000:.1f}s). "
                "Consider caching compiled models with torch.export or torch.save."
            )
        
        if not recommendations:
            recommendations.append("No significant issues detected.")
        
        return recommendations
    
    def compare_modes(
        self,
        model_or_fn: Callable,
        sample_inputs: Any,
        modes: Optional[List[str]] = None,
    ) -> Dict[str, CompileReport]:
        """
        Compare different torch.compile modes.
        
        Args:
            model_or_fn: Model or function to analyze
            sample_inputs: Sample inputs
            modes: List of modes to compare (default: all standard modes)
            
        Returns:
            Dict mapping mode name to CompileReport
        """
        modes = modes or ["default", "reduce-overhead", "max-autotune"]
        results = {}
        
        for mode in modes:
            self.mode = mode
            try:
                results[mode] = self.analyze(model_or_fn, sample_inputs)
            except Exception as e:
                report = CompileReport(mode=mode)
                report.recommendations = [f"Mode failed: {e}"]
                results[mode] = report
        
        return results
    
    def export_report(
        self,
        report: CompileReport,
        path: Path,
        format: str = "json",
    ):
        """Export compile report to file."""
        path = Path(path)
        
        if format == "json":
            path.write_text(json.dumps(report.to_dict(), indent=2))
        
        elif format == "html":
            html = self._generate_html_report(report)
            path.write_text(html)
    
    def _generate_html_report(self, report: CompileReport) -> str:
        """Generate HTML report."""
        speedup_color = "#22c55e" if report.speedup >= 1.2 else "#f59e0b" if report.speedup >= 1.0 else "#ef4444"
        
        return f'''<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>torch.compile Analysis</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background: #0f172a;
            color: #e2e8f0;
        }}
        h1, h2 {{ margin-bottom: 15px; }}
        .section {{
            background: #1e293b;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
        }}
        .metric {{
            display: inline-block;
            padding: 15px 25px;
            background: #334155;
            border-radius: 8px;
            margin: 5px;
            text-align: center;
        }}
        .metric-value {{
            font-size: 28px;
            font-weight: bold;
        }}
        .metric-label {{
            font-size: 12px;
            color: #94a3b8;
        }}
        .break-item {{
            background: #3f1f1f;
            border-left: 4px solid #ef4444;
            padding: 10px 15px;
            margin: 8px 0;
            border-radius: 0 8px 8px 0;
        }}
        .recommendation {{
            background: #1e3a5f;
            border-left: 4px solid #3b82f6;
            padding: 10px 15px;
            margin: 8px 0;
            border-radius: 0 8px 8px 0;
        }}
    </style>
</head>
<body>
    <h1>⚡ torch.compile Analysis</h1>
    
    <div class="section">
        <h2>Performance</h2>
        <div class="metric">
            <div class="metric-value" style="color: {speedup_color}">{report.speedup:.2f}x</div>
            <div class="metric-label">Speedup</div>
        </div>
        <div class="metric">
            <div class="metric-value">{report.eager_time_ms:.2f}ms</div>
            <div class="metric-label">Eager Time</div>
        </div>
        <div class="metric">
            <div class="metric-value">{report.compiled_time_ms:.2f}ms</div>
            <div class="metric-label">Compiled Time</div>
        </div>
        <div class="metric">
            <div class="metric-value">{report.compile_time_ms/1000:.1f}s</div>
            <div class="metric-label">Compile Time</div>
        </div>
    </div>
    
    <div class="section">
        <h2>Fusion Analysis</h2>
        <div class="metric">
            <div class="metric-value">{report.ops_before_fusion}</div>
            <div class="metric-label">Ops Before</div>
        </div>
        <div class="metric">
            <div class="metric-value">{report.ops_after_fusion}</div>
            <div class="metric-label">Ops After</div>
        </div>
        <div class="metric">
            <div class="metric-value">{report.fusion_ratio:.1f}x</div>
            <div class="metric-label">Fusion Ratio</div>
        </div>
    </div>
    
    {f'''<div class="section">
        <h2>Graph Breaks ({report.total_graph_breaks})</h2>
        {''.join(f'<div class="break-item"><strong>{b.reason}</strong><br><em>{b.suggestion}</em></div>' for b in report.graph_breaks[:5])}
    </div>''' if report.graph_breaks else ''}
    
    <div class="section">
        <h2>Recommendations</h2>
        {''.join(f'<div class="recommendation">{r}</div>' for r in report.recommendations)}
    </div>
</body>
</html>'''



