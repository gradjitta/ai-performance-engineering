# ðŸ“š Optimization Explanation: stream_ordered

**Technique:** CUDA Graph Capture with Multi-Stream Parallelism
**Variant:** llm_opt_v4
**Speedup Achieved:** 3.27x

## What Changed?

The optimization pre-captures the neural network inference operations into CUDA Graphs during setup, then replays these captured graphs across multiple streams during benchmarking. Instead of launching individual CUDA kernels with their associated CPU overhead each iteration, the entire sequence of operations is recorded once and replayed as a single unit, dramatically reducing kernel launch latency while maintaining parallel execution across streams.

## Why It Works

On NVIDIA Blackwell B200 GPUs, kernel launch overhead from the CPU can become a significant bottleneck for small, fast kernels like those in this lightweight neural network. CUDA Graphs eliminate per-kernel launch overhead by bundling multiple operations into a single launchable unit. Combined with 4 concurrent streams, this allows the GPU to overlap memory transfers (H2D copies of inputs, D2H copies of outputs) with compute operations across different requests, maximizing hardware utilization. The B200's advanced scheduling hardware can efficiently manage multiple concurrent graph executions.

## Key Concepts to Understand

- CUDA Graphs: Pre-recorded sequences of GPU operations that can be launched with minimal CPU overhead, reducing the typical ~5-10Î¼s per kernel launch to nearly zero for the captured sequence
- Multi-Stream Parallelism: Using multiple CUDA streams allows independent operations to execute concurrently, overlapping memory transfers with computation and hiding latency
- Pinned Memory: Host tensors use pin_memory=True, enabling asynchronous DMA transfers that don't block the CPU and can proceed in parallel with other operations
- Static Buffer Reuse: Pre-allocated static_inputs and static_outputs buffers avoid allocation overhead during graph replay and ensure memory addresses remain constant for the captured graph

## Performance Impact

- **Memory Bandwidth:** Improved utilization through overlapped H2D and D2H transfers across streams - while one stream computes, others can be transferring data, better saturating PCIe bandwidth
- **Compute Utilization:** Higher SM occupancy as graph replay has near-zero launch gaps between kernels, keeping tensor cores and CUDA cores continuously fed with work
- **Latency:** 3.27x reduction primarily from eliminating per-kernel CPU launch overhead - each graph replay replaces multiple individual kernel launches with one optimized submission

## When to Use This Technique

Apply CUDA Graphs when you have repetitive workloads with fixed tensor shapes and operation sequences, especially when processing many small batches or requests where kernel launch overhead dominates. Ideal for inference serving, real-time applications, and any scenario where the same computation pattern repeats thousands of times.

## When NOT to Use This Technique

Avoid CUDA Graphs when tensor shapes vary between iterations (graphs require fixed shapes), when operations have dynamic control flow that changes per-iteration, during model development/debugging (graphs obscure individual kernel behavior), or when workloads are already compute-bound with large batch sizes where launch overhead is negligible.

## Further Reading

- NVIDIA CUDA Graphs documentation and best practices
- Asynchronous CUDA stream programming and stream concurrency
- PyTorch CUDA Graph integration (torch.cuda.CUDAGraph)
- Memory transfer optimization with pinned memory and async copies
- NVIDIA Nsight Systems for visualizing stream concurrency and graph execution

---
*Generated by LLM-powered benchmark analysis*
