"""optimized_paged_attn_vllm.py - Chunked/paged attention stand-in.

Imitates vLLM-style paged attention by processing KV in blocks and reusing a
small cache tensor. Keeps dependencies minimal so it can run in the harness.
"""

from __future__ import annotations

import sys
from contextlib import nullcontext
from pathlib import Path
from typing import Optional

import torch
import torch.nn.functional as F

repo_root = Path(__file__).parent.parent
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from common.python.benchmark_harness import BaseBenchmark, WorkloadMetadata  # noqa: E402
from common.python.nvtx_helper import get_nvtx_enabled, nvtx_range  # noqa: E402

# Use new SDPA API when available
try:
    from torch.nn.attention import sdpa_kernel, SDPBackend
    _FLASH_BACKENDS = [SDPBackend.FLASH_ATTENTION]
    _NEW_SDPA_API = True
except ImportError:
    sdpa_kernel = None  # type: ignore[assignment]
    SDPBackend = None  # type: ignore[assignment]
    _FLASH_BACKENDS = []
    _NEW_SDPA_API = False


def _flash_sdpa_context():
    """Return context manager for flash attention backend."""
    if _NEW_SDPA_API and sdpa_kernel is not None:
        return sdpa_kernel(_FLASH_BACKENDS)
    return nullcontext()


class OptimizedPagedAttnBenchmark(BaseBenchmark):
    def __init__(self) -> None:
        super().__init__()
        self.qkv: Optional[torch.Tensor] = None
        self._workload = WorkloadMetadata(tokens_per_iteration=0.0)

    def setup(self) -> None:
        torch.manual_seed(1)
        # Longer sequence to expose flash SDPA advantage (O(N) vs O(NÂ²) memory).
        b, h, s, d = 4, 16, 2048, 64
        # Optimized path keeps BF16 and will force flash SDPA.
        self.qkv = torch.randn(b, h, s, 3, d, device=self.device, dtype=torch.bfloat16)
        # Aggressive warmup: run flash SDPA multiple times to fully JIT-compile.
        # Flash attention has heavier first-call overhead than the math path.
        q = self.qkv[:, :, :, 0]
        k = self.qkv[:, :, :, 1]
        v = self.qkv[:, :, :, 2]
        with _flash_sdpa_context():
            for _ in range(10):
                _ = F.scaled_dot_product_attention(q, k, v)
                torch.cuda.synchronize(self.device)

    def benchmark_fn(self) -> Optional[dict]:
        if self.qkv is None:
            raise RuntimeError("SKIPPED: paged attention buffers not initialized")
        q = self.qkv[:, :, :, 0]
        k = self.qkv[:, :, :, 1]
        v = self.qkv[:, :, :, 2]

        enable_nvtx = get_nvtx_enabled(self.get_config())
        # Force flash SDPA to highlight the fused path.
        with _flash_sdpa_context():
            with nvtx_range("paged_attn_vllm", enable=enable_nvtx):
                _ = F.scaled_dot_product_attention(q, k, v)
        torch.cuda.synchronize(self.device)
        return {}

    def get_workload_metadata(self) -> Optional[WorkloadMetadata]:
        return self._workload


    def get_custom_metrics(self) -> Optional[dict]:
        """Return speculative decoding metrics."""
        return {
            "paged_attn_vllm.num_draft_tokens": float(getattr(self, 'num_draft_tokens', 4)),
            "paged_attn_vllm.batch_size": float(getattr(self, 'batch_size', 1)),
        }

def get_benchmark() -> BaseBenchmark:
    return OptimizedPagedAttnBenchmark()
